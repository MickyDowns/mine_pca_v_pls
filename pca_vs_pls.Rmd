---
title: "pca vs. pls"
output: html_document
---

```{r}
load("body.RData")
library(pls)

# a
plot(Y$Height~Y$Weight,col=Y$Gender+1,pch=Y$Gender+1,main="Height~Weight|Gender")
legend("topleft", col = c(2,1), pch = c(2, 1), legend = c("likely male","likely female"))

# b
# Reserve 200 observations from your dataset to act as a test set and use the remaining 307 as a training set. 
set.seed(123)
test = sort(sample(1:nrow(X), 200))
train = (1:nrow(X))[-test]

# On the training set, use both pcr and plsr fit models to predict a person's weight based on the variables in X. Use the options scale=TRUE and validation=`CV'. Why does it make sense to scale our variables in this case?

pcr.fit=pcr(Y$Weight~.,data=X,scale=TRUE,validation="CV",subset=train)
plsr.fit=plsr(Y$Weight~.,data=X,scale=TRUE,validation="CV",subset=train)
pcr.fit.prin=princomp(X,scale=TRUE,validation="CV",subset=train)

# c
# Run summary() on each of the objects calculated above, and compare the training % variance explained from the pcr output to the plsr output. Do you notice any consistent patterns (in comparing the two)? Is that pattern surprising? Explain why or why not.

summary(pcr.fit.1);summary(plsr.fit);summary(pcr.fit.prin)

pcr.pct.x=c(63.93   ,  75.0  ,  79.71  ,  83.99 ,   86.40    ,88.34   , 89.94 ,   91.42 ,   92.74, 93.90 ,    94.91  ,   95.83  ,   96.64  ,    97.4   ,  98.03  ,   98.53  ,   98.95, 99.30  ,   99.58  ,   99.82   , 100.00)

pcr.pct.y=c(94.53  ,   94.9 ,   95.61   , 95.61   , 95.72    ,95.85  ,  95.88  ,  95.99  ,  96.00  , 96.02   ,  96.04   ,  96.05    , 96.13   ,   96.2  ,   96.26   ,  96.42   ,  96.59,   96.65  ,   96.65   ,  96.65  ,   96.65)

plsr.pct.x=c(63.92  ,  72.82  ,  79.04  ,  81.19  ,  82.58  ,  84.10   , 85.60  ,  88.69  ,  90.74 ,91.51    , 92.45    , 93.36   ,  94.34 ,    95.12     ,95.95  ,   96.77   ,  97.59, 98.13   ,  98.79  ,   99.44  ,  100.00)

plsr.pct.y=c(94.78  ,  95.77   , 96.17   , 96.48  ,  96.61  ,  96.64   , 96.64  ,  96.65,    96.65, 96.65,     96.65 ,    96.65 ,    96.65   ,  96.65   ,  96.65   ,  96.65   ,  96.65, 96.65   ,  96.65  ,   96.65   ,  96.65)

pcr.pct.prin.x=c(0.7530, 0.8803, 0.91899, 0.93859, 0.95239, 0.961843, 0.970273, 0.976086, 0.980916, 0.985128, 0.988343, 0.990659, 0.992833, 0.994545, 0.996057, 0.997424, 0.998286, 0.9988784, 0.9993680, 0.9997574, 1.0000000000)

par(mfrow=c(1,1))
plot(x_bot,pcr.pct.x,type="b",col="darkblue",ylab="% variance explained",cex=0.5,
     xlab="# of components",main="portion of variance explained")
lines(x_bot,plsr.pct.x,type="b",col="red4",cex=0.5)
lines(x_bot,pcr.pct.y,type="b",col="blue",cex=0.5,pch=2)
lines(x_bot,plsr.pct.y,type="b",col="red1",cex=0.5,pch=2)
lines(x_bot,100*pcr.pct.prin.x,type="b",col="black",cex=0.5)

legend("bottom", col=c("darkblue","red4","blue","red1","black"),
       pch=c(1,2,1,2,1),legend=c("pcr:x","plsr:x","pcr:y","plsr:y","princom:x"))

#d 
## plot train MSE as function of components 
par(mfrow=c(1,2))
validationplot(pcr.fit,val.type="MSEP",xlab="# of components",main="pcr() components")
validationplot(plsr.fit,val.type="MSEP",xlab="# of components",main="plsr()components ")

## 
set.seed(123)
test.feat = sort(sample(1:nrow(X[test,]), 100))
test.cv= (1:nrow(X[test,]))[-test.feat]

pcr.mse.test=NULL
plsr.mse.test=NULL
for(i in 1:21){
     pcr.pred=predict(pcr.fit,X[test.feat,],ncomp=i)
     pcr.mse.test[i]=mean((pcr.pred-Y$Weight[test.feat])^2)
     
     plsr.pred=predict(plsr.fit,X[test.feat,],ncomp=i)
     plsr.mse.test[i]=mean((plsr.pred-Y$Weight[test.feat])^2)
}

par(mfrow=c(1,1))
plot(x_bot,pcr.mse.test,type="b",col="darkblue",ylab="MSE",cex=0.5,
     xlab="# of components",main="comparison of test mse by component")
lines(x_bot,plsr.mse.test,type="b",col="red4",cex=0.5)

legend("topright", col=c("darkblue","red4"),
       pch=c(1,2,3),legend=c("pcr  ","plsr  "))

## find lowes number of dimensions w/in 1 sd of minimum MSE
min(x_bot[pcr.mse.test<min(pcr.mse.test)+sd(pcr.mse.test)])
min(x_bot[plsr.mse.test<min(plsr.mse.test)+sd(plsr.mse.test)])

# e
## alternative 1: pcr(), plsr()
par(mfrow=c(1,3))
# pcr() and plsr() substantially agree loadings for components 1:3
plot(pcr.fit$loadings[,1],type="b",col="blue",ylab="component 1")
lines(plsr.fit$loadings[,1],type="b",col="red")

plot(pcr.fit$loadings[,2],type="b",col="blue",ylab="component 2")
lines(plsr.fit$loadings[,2],type="b",col="red")

plot(pcr.fit$loadings[,3],type="b",col="blue",ylab="comonent 3")
lines(plsr.fit$loadings[,3],type="b",col="red")
mtext("pcr() and plsr() loading similarity", side = 3, line = -2.5, outer = TRUE)

## try using fits generated to date, but only taking one measurement going forward
mse.test=matrix(NA,21,3)
rownames(mse.test)=rownames(pcr.fit$loadings)
colnames(mse.test)=c("components","pcr_mse","plsr_mse")

for(i in 1:21){
          mse.test[i,1]=1
          pcr.pred= pcr.fit$scores[i,1]*X[test.feat,i]
          mse.test[i,2]=mean((pcr.pred-Y$Weight[test.feat])^2)

          plsr.pred=plsr.fit$scores[i,1] * X[test.feat,i]
          mse.test[i,3]=mean((plsr.pred-Y$Weight[test.feat])^2)
}
mse.test[order(mse.test[,2],decreasing=FALSE),]

# alternative 2: lasso
library(glmnet)
grid=10^seq(10,-2,length=100)
lasso.mod=glmnet(as.matrix(X[train,]),Y$Weight[train],alpha=1,lambda=grid)
par(mfrow=c(1,2))
plot(lasso.mod)
dim(coef(lasso.mod))
lasso.mod$lambda[50]
predict(lasso.mod,s=15,type="coefficients")[1:22,]

set.seed(1)
cv.out=cv.glmnet(as.matrix(X[train,]),Y$Weight[train],alpha=1)
plot(cv.out)
abline(h=66,col="blue")
abline(v=1.9,col="blue")
mtext("5 lasso parms match MSE performance of 1st component", side = 3, line = -1.5, outer = TRUE)

# f
pcr.pred=predict(pcr.fit,X[test.cv,],ncomp=3)
pcr.mse.test=mean((pcr.pred-Y$Weight[test.cv])^2)
     
plsr.pred=predict(plsr.fit,X[test.cv,],ncomp=2)
plsr.mse.test=mean((plsr.pred-Y$Weight[test.cv])^2)

bestlam=cv.out$lambda.min
lasso.pred=predict(lasso.mod,s=bestlam,newx=as.matrix(X[test.cv,]))
mean((lasso.pred-Y$Weight[test.cv])^2)

out=glmnet(as.matrix(X[test.cv,]),Y$Weight[test.cv],alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:22]
lasso.coef

```



